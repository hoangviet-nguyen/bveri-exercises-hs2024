{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c6b40",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b703d4d-0ab2-41cc-b8c4-3f42d2409dc8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Object-Detection\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Object classification and localization: Understand how objects can be localized using an example with restricted complexity.\n",
    "- Object detection: Use and understand a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8a531-576a-4f94-bfc8-e6865636f36c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's define paths, install & load the necessary Python packages.\n",
    "\n",
    "**Optional: Save the notebook to your personal google drive to persist changes.**\n",
    "\n",
    "**Optional: Change runtime to a GPU instance (if using Google Colab)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f9241-d1e5-4ea8-9a03-72356c105e8f",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results (if running the code in Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0afc1-c839-4511-af51-2a130c164c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82574179-d633-4cbb-9108-4af85b1e2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9198a-773e-4620-b7e5-661e489a3c95",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab87e3-f8f8-47b4-9c59-cfe9020f2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/bveri\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"/workspace/code/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86657df5-007d-4c6f-9147-fa905923b531",
   "metadata": {},
   "source": [
    "Install `dl_cv_lectures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced84ee-42e6-4b68-9f8b-a8f55cf0217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dl_cv_lectures\n",
    "\n",
    "    print(\"dl_cv_lectures installed, all good\")\n",
    "except ImportError as e:\n",
    "    import os\n",
    "\n",
    "    if Path(\"/workspace/code/src\").exists():\n",
    "        print(\"Installing from local repo\")\n",
    "        os.system(\"cd /workspace/code  && pip install -e .\")\n",
    "    else:\n",
    "        print(\"Installing from git repo\")\n",
    "        os.system(\"pip install git+https://github.com/i4Ds/bveri-exercises-hs2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913b54c-d941-4e66-84e0-227eee86dea7",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca04432-2b2f-4655-b425-9974887c83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchshow as ts\n",
    "import torchvision\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dl_cv_lectures import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6842f-8fe3-454d-b445-0a1aeffa5293",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c09743-bccf-41b3-af72-76e21965929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96e630-7583-4979-981e-a1ef8dc62efb",
   "metadata": {},
   "source": [
    "## Object Classification and Localization\n",
    "\n",
    "We simplify the task: We classify and localize jut one object per image. This demonstrates how object detection works and illustrates the fundamental challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f171d3-fd72-40b1-ba2a-4d70e44c01a9",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We create an artificial dataset. The following class creates images and labels on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd82cb5-8d6b-49f9-8599-63f0d1afbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data import shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3b50f-0e11-4d3b-8654-5eece0a52914",
   "metadata": {},
   "source": [
    "Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646e1df-8a3c-408d-b959-25cb32518d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "ds = shapes.ShapeDataset(img_size=64, max_number_of_shapes_per_image=1, background=\"white\")\n",
    "\n",
    "\n",
    "def get_images_and_labels_from_ds(\n",
    "    ds: torch.utils.data.Dataset, num_images_to_fetch: int = 16\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Fetch first n images from a torch.utils.data.Dataset with (image, label) signature.\"\"\"\n",
    "    # for each image: convert it to (N x C x H x W) format and scale to 0-1\n",
    "    images = [\n",
    "        TF.to_image(ds[i][0]).to(torch.float32).unsqueeze(0) / 255.0\n",
    "        for i in range(0, num_images_to_fetch)\n",
    "    ]\n",
    "    labels = [ds[i][1] for i in range(0, num_images_to_fetch)]\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "images, labels = get_images_and_labels_from_ds(ds, num_images_to_fetch=16)\n",
    "\n",
    "images_to_plot = list()\n",
    "for image, label in zip(images, labels):\n",
    "    boxes = torch.tensor(label[\"box\"])\n",
    "    image = (image * 255.0).to(torch.uint8).squeeze(0)\n",
    "    img_with_box = draw_bounding_boxes(image=image, boxes=boxes)\n",
    "    images_to_plot.append(img_with_box)\n",
    "\n",
    "\n",
    "fig, ax = visualize.plot_square_collage(images_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df18ed-60ab-4b28-a9ac-f30704fe2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.show(ds[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4446653-6ef3-4a09-ab9c-e423704c1847",
   "metadata": {},
   "source": [
    "**Question:** What do you see in the outputs above? What is in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f765aad-f5e4-4ebb-bf61-9c948569c321",
   "metadata": {},
   "source": [
    "### Create a Training-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d84c4-f085-4500-bad4-cec18d2c07ef",
   "metadata": {},
   "source": [
    "First, we look at the output of the `DataLoader` object. Thats what the model needs to process.\n",
    "\n",
    "Check the outputs for their data type and shape.\n",
    "\n",
    "`collate_fn` defines how samples are batches. This is a particular head ache in object detection since each image has a varying number of objects: [Link](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aa212-8186-4b5b-b9f5-45b35d02b570",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ab655ef85a9c58c5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.5, 1.0)])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    return torch.stack(images), annotations\n",
    "\n",
    "\n",
    "dataset = shapes.ShapeDataset(\n",
    "    img_size=64,\n",
    "    num_samples=1000,\n",
    "    background=\"white\",\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    transforms=transform,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "images, annotations = next(iter(dataloader))\n",
    "\n",
    "images.shape\n",
    "annotations[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76341cc3-0036-4c03-891d-ade8080aa718",
   "metadata": {},
   "source": [
    "**Question:** How does the output of `ShapeDataset` with a `DataLoader` look like? What are the difficulties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066c867-8209-4488-acc1-679324dc740c",
   "metadata": {},
   "source": [
    "### Defining the Architecture\n",
    "\n",
    "We will now define an architecture consisting of three components:\n",
    "\n",
    "- **Backbone**: A CNN for feature extraction  \n",
    "- **Classification Head**: Models the classification task  \n",
    "- **Bounding Box Regression Head**: Models the bounding box regression  \n",
    "\n",
    "Create the backbone with 3 convolutional layers:  \n",
    "\n",
    "- **Conv Layer 1**: 16 filters, stride 1  \n",
    "- **Conv Layer 2**: 32 filters, stride 2  \n",
    "- **Conv Layer 3**: 64 filters, stride 2  \n",
    "\n",
    "Add layers for the heads accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f429a-0b52-4e40-ac52-26143a9003d2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-61e829411a3557bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CNNBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Backbone for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple[int, int]): The height and width of the input images.\n",
    "        output_features (int): The number of features to output after the fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int] = (64, 64), output_features: int = 128):\n",
    "        super(CNNBackbone, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        ### BEGIN SOLUTION\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Calculate the flattened feature size after convolutions\n",
    "        global_stride = 4  # Total downsampling factor from stride 2 in conv2 and conv3\n",
    "        cnn_features = 64 * (input_shape[0] // global_stride) * (input_shape[1] // global_stride)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(cnn_features, output_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the CNN backbone.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Extracted feature tensor of shape (batch_size, output_features).\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classification head for predicting class logits.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes for classification.\n",
    "        num_input_features (int): Number of input features from the backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, num_input_features: int):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.fc2_class = nn.Linear(num_input_features, num_classes)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the classification head.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_input_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Class logits of shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "        class_logits = self.fc2_class(x)\n",
    "        return class_logits\n",
    "\n",
    "\n",
    "class DetectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Detection head for predicting bounding box coordinates.\n",
    "\n",
    "    Args:\n",
    "        num_input_features (int): Number of input features from the backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_input_features: int):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        ### BEGIN SOLUTION\n",
    "        self.fc2_bb = nn.Linear(num_input_features, 4)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the detection head.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_input_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bounding box coordinates of shape (batch_size, 4).\n",
    "        \"\"\"\n",
    "        bb_coords = self.fc2_bb(x)\n",
    "        return bb_coords\n",
    "\n",
    "\n",
    "class ObjectClassificationAndLocalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined model for object classification and localization.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple[int, int]): The height and width of the input images.\n",
    "        num_features (int): Number of features output by the backbone.\n",
    "        num_classes (int): Number of classes for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int], num_features: int, num_classes: int):\n",
    "        super(ObjectClassificationAndLocalization, self).__init__()\n",
    "        self.backbone = CNNBackbone(input_shape=input_shape, output_features=num_features)\n",
    "        self.classification_head = ClassificationHead(num_classes, num_features)\n",
    "        self.detection_head = DetectionHead(num_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the combined model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - Class scores of shape (batch_size, num_classes).\n",
    "                - Bounding box coordinates of shape (batch_size, 4).\n",
    "        \"\"\"\n",
    "        x = self.backbone(x)\n",
    "        class_scores = self.classification_head(x)\n",
    "        bb_coords = self.detection_head(x)\n",
    "        return class_scores, bb_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb7c1b-e6fb-45f3-bdb1-aadb0cc5d44c",
   "metadata": {},
   "source": [
    "### Initialize Model, Loss Function, and Optimizer\n",
    "\n",
    "Initialize your model. Define loss functions (separate ones for classification and regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd437c-c504-42c7-a9b2-07e3dba911ff",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b45f32300f92c0db",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "net = ObjectClassificationAndLocalization(input_shape=(64, 64), num_features=128, num_classes=3)\n",
    "\n",
    "# define loss functions for the different heads\n",
    "# loss_fn_class = nn.\n",
    "# loss_fn_bbx = nn.\n",
    "### BEGIN SOLUTION\n",
    "loss_fn_class = nn.CrossEntropyLoss()\n",
    "loss_fn_bbx = nn.MSELoss()\n",
    "### END SOLUTION\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ac37b-9740-4a8f-a10d-3780290dc957",
   "metadata": {},
   "source": [
    "### Modell Training\n",
    "\n",
    "Let's train the model for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ac9ea-b009-43d5-9c6f-1462bdfc79f2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-992747d62f6b7322",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_cls_loss = 0.0\n",
    "    running_bbx_loss = 0.0\n",
    "    for step, (images, annotations) in enumerate(dataloader):\n",
    "        labels_class = torch.tensor([sample[\"class\"][0] for sample in annotations])\n",
    "        labels_bb = torch.tensor([sample[\"box\"][0] for sample in annotations])\n",
    "\n",
    "        # scale bb labels\n",
    "        labels_bb_scaled = labels_bb / images.shape[2]\n",
    "\n",
    "        # Forward pass\n",
    "        class_scores, bb_coords = net(images)\n",
    "\n",
    "        # compute classification and regression losses\n",
    "        # loss_class = ...\n",
    "        # loss_bb = ...\n",
    "        ### BEGIN SOLUTION\n",
    "        # Compute classification loss\n",
    "        loss_class = loss_fn_class(class_scores, labels_class)\n",
    "\n",
    "        # Compute bounding box regression loss\n",
    "        loss_bb = loss_fn_bbx(bb_coords, labels_bb_scaled.float())\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Total loss (you can adjust weights for classification and regression losses)\n",
    "        # total_loss = ...\n",
    "        ### BEGIN SOLUTION\n",
    "        total_loss = loss_class + 10 * loss_bb\n",
    "        ### END SOLUTION\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_cls_loss += loss_class.item()\n",
    "        running_bbx_loss += loss_bb.item()\n",
    "        running_loss += loss_class.item() + loss_bb.item()\n",
    "\n",
    "    print(\n",
    "        f\"epoch: {epoch + 1:02d} - step: {step + 1:5d} - total loss: {running_loss / (step+1):.3f} \\\n",
    "        Class Loss: {running_cls_loss  / (step+1):.4f} BB Loss: {running_bbx_loss / (step+1) :.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625f214-1dd9-4323-92fb-35eff8ab2853",
   "metadata": {},
   "source": [
    "**Question:** What do you notice when looking at the loss-values? What can you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca91806-0094-466b-a423-5855349112a1",
   "metadata": {},
   "source": [
    "### Modell Evaluation\n",
    "\n",
    "Now we evaluate the model (using simple means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b2aae-dd3a-4b95-b1bd-94e7be18f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = shapes.ShapeDataset(\n",
    "    background=\"white\",\n",
    "    max_number_of_shapes_per_image=1,\n",
    "    img_size=64,\n",
    "    num_samples=256,\n",
    "    transforms=transform,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(dataset, batch_size=256, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a5639-7fe3-4557-b9e6-afd071f6f350",
   "metadata": {},
   "source": [
    "Let's take a look at the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a7a42-f4d8-45f9-b722-c3f3d6890ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    net = net.eval()\n",
    "    class_scores, bb_coords = net(test_images)\n",
    "\n",
    "y_class_pred = torch.argmax(class_scores, 1).numpy()\n",
    "y_class_true = np.array([sample[\"class\"][0] for sample in test_labels])\n",
    "\n",
    "for i, (y_pred, y_true) in enumerate(zip(y_class_pred, y_class_true)):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(f\"Predicted: {test_dataset.classes[y_pred]} True: {test_dataset.classes[y_true]}\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {sum(y_class_pred == y_class_true) / len(y_class_true)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e0bae-6620-43d0-b16d-f1fea50d7f03",
   "metadata": {},
   "source": [
    "Now we vsualize prediction and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c38bc-49f4-4a66-84ce-d41544b874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = shapes.ShapeDataset(\n",
    "    img_size=64, max_number_of_shapes_per_image=1, background=\"white\", transforms=transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "for i, (img, annotations) in enumerate(test_dataloader):\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        class_scores, bb_coords_scaled = net(img)\n",
    "        bb_coords = bb_coords_scaled * img.shape[2]\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    img = img.squeeze(0)\n",
    "\n",
    "    img = (img * 2 * 255.0).to(torch.uint8)\n",
    "    boxes = torch.tensor(annotations[0][\"box\"])\n",
    "    img_with_box = draw_bounding_boxes(image=img, boxes=boxes, colors=\"red\")\n",
    "\n",
    "    # Plot Prediction\n",
    "    img_with_box = draw_bounding_boxes(image=img_with_box, boxes=bb_coords, colors=\"green\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    pil_image = F.to_pil_image(img_with_box)\n",
    "    _ = ax.imshow(pil_image)\n",
    "    # Remove x and y axis ticks\n",
    "    _ = ax.set_xticks([])\n",
    "    _ = ax.set_yticks([])\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d0f91-6324-4932-9fbe-1dc763c72c90",
   "metadata": {},
   "source": [
    "**Question:** Qualitatively: How do you like your model with respect to classification and localization performance? Where could you improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ca539-729a-401c-a8e6-13b6e391861b",
   "metadata": {},
   "source": [
    "The following code can be used to evaluate the detection performance by calculating IoU.\n",
    "\n",
    "Complete the code. Use [torchvision.ops.box_iou](https://pytorch.org/vision/main/generated/torchvision.ops.box_iou.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3db1e2-df8f-44c4-9fe1-a7e940884ba1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8f488a9b108684ea",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = shapes.ShapeDataset(\n",
    "    img_size=64, max_number_of_shapes_per_image=1, background=\"white\", transforms=transform\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "for i, (img, annotations) in enumerate(test_dataloader):\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        net = net.eval()\n",
    "        class_scores, bb_coords_scaled = net(img)\n",
    "        predicted_box = bb_coords_scaled * img.shape[2]\n",
    "\n",
    "    gt_box = torch.tensor(annotations[0][\"box\"]).reshape(1, -1)\n",
    "\n",
    "    # iou = ...\n",
    "    ### BEGIN SOLUTION\n",
    "    iou = torchvision.ops.box_iou(gt_box, predicted_box).numpy().ravel()[0]\n",
    "    ### END SOLUTION\n",
    "    print(f\"IOU: {iou:.2f}\")\n",
    "\n",
    "    # Plot Ground Truth\n",
    "    img = img.squeeze(0)\n",
    "\n",
    "    img = (img * 2 * 255.0).to(torch.uint8)\n",
    "    boxes = torch.tensor(annotations[0][\"box\"])\n",
    "    img_with_box = draw_bounding_boxes(image=img, boxes=boxes, colors=\"red\")\n",
    "\n",
    "    # Plot Prediction\n",
    "    img_with_box = draw_bounding_boxes(image=img_with_box, boxes=predicted_box, colors=\"green\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2, 2))\n",
    "    pil_image = F.to_pil_image(img_with_box)\n",
    "    _ = ax.imshow(pil_image)\n",
    "    # Remove x and y axis ticks\n",
    "    _ = ax.set_xticks([])\n",
    "    _ = ax.set_yticks([])\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810200-6ec1-4ba9-9ac6-b99452867eac",
   "metadata": {},
   "source": [
    "**Question:** Which IoUs do you find acceptable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c262042-a682-4253-896d-d587953cf010",
   "metadata": {},
   "source": [
    "**Question:** What would you need to adjust to perform object detection (with multiple objects per image)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fb9a2-5d8f-42c9-9835-e9ac81024e7b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-edcdcb36b38944ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Pre-Trained _Faster R-CNN_\n",
    "\n",
    "In this exercise we will use a pre-trained object detection model from the family of _Faster R-CNNs_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb746ed-86b4-4203-808a-767ef046a91f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ad02c611cb6b0d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "Load the following images with `PIL.Image`. Inspect the images and estimate how well object detection algorithms might perform.\n",
    "\n",
    "```\n",
    "DATA_PATH.joinpath(\"dogs.jpg\")\n",
    "DATA_PATH.joinpath(\"ducks.jpeg\")\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb29c5-7b7c-4018-aee6-5ef7ade2b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "files = [\n",
    "    {\"id\": \"18zuHwfojUUpmkrQttEtuaNW-MQ0QOoAH\", \"name\": \"ducks.jpg\"},\n",
    "    {\"id\": \"1-UWVWqTpE80Qxh36hPuKkuQZj5BT3hXr\", \"name\": \"dogs.jpg\"},\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    url = f\"https://drive.google.com/uc?id={file['id']}\"\n",
    "    download_path = DATA_PATH / file[\"name\"]\n",
    "    if not download_path.exists():\n",
    "        gdown.download(url, str(download_path), quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84076903-a758-45e3-8c19-f33abeed0e81",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fe920be22a3799de",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dogs = Image.open(DATA_PATH.joinpath(\"dogs.jpg\"))\n",
    "img_ducks = Image.open(DATA_PATH.joinpath(\"ducks.jpg\"))\n",
    "\n",
    "display(img_ducks)\n",
    "display(img_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519ef56-3c75-4ab9-b9c9-74d87f9683a8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b62acd4ad33420df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Load Model\n",
    "\n",
    "Load a pre-trained model of the _Faster R-CNN_ family from [torchvision](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection). For example, you could load `fasterrcnn_mobilenet_v3_large_320_fpn`, which is resource-efficient. If you want better performance you might choose a different one.\n",
    "\n",
    "Initialize the model and put it into `eval` mode. Set `box_score_thresh` to a value between 0.5 und 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0a5e-95a0-4914-b1da-802f79724cd4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f757fea1c417faf8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    ")\n",
    "\n",
    "Model = fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "weights = FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT\n",
    "\n",
    "# Model = fasterrcnn_resnet50_fpn\n",
    "# weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "\n",
    "model = Model(weights=weights, box_score_thresh=0.8)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b61f022-78fb-4b4f-a350-7645b20aba33",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc2e5e0d131aa594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Use Model\n",
    "\n",
    "Use the function  `inference_single()` to create predictions for an image. Take a look at the following example: https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\n",
    "\n",
    "Create predictions for \"dogs.jpg\" and insepct the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e0874-33cd-4ddc-8322-fb8593293d1d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eafdeb3e614c1507",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_single(img, model, preprocess):\n",
    "    \"\"\"Inference on a single image\n",
    "    Args:\n",
    "        img: (C, H, W) torch.tensor\n",
    "        model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "        preprocess: function to pre-process image batch for the model\n",
    "\n",
    "    Returns:\n",
    "        predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "\n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "    return model(image_processed)[0]\n",
    "\n",
    "\n",
    "img = TF.pil_to_tensor(img_dogs)\n",
    "### BEGIN SOLUTION\n",
    "predictions = inference_single(img, model, weights.transforms())\n",
    "predictions\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9c3ae-aef5-4918-8350-817d60d85af1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbdc45817692565a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Visualize the predictions using [torchvision.utils.draw_bounding_boxes](torchvision.utils.draw_bounding_boxes). \n",
    "\n",
    "Visualize the labels and the confidence scores of the predictions together with the bounding boxes.\n",
    "\n",
    "The labels are in `weights.meta[\"categories\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331a863-9f13-4f64-b99d-7283ce457fb3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1ff57e859223906",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "def draw_boxes(image, predictions, categories):\n",
    "    \"\"\"Draw Boxes from Predictions\n",
    "    Args:\n",
    "        image: The input image torch.tensor\n",
    "        predictions: Output of inference()\n",
    "        categories: List of category labels\n",
    "    Returns:\n",
    "        PIL.Image\n",
    "    \"\"\"\n",
    "    labels = [\n",
    "        f\"{categories[i]} ({s * 100:.2f} %)\"\n",
    "        for i, s in zip(predictions[\"labels\"], predictions[\"scores\"])\n",
    "    ]\n",
    "\n",
    "    box = draw_bounding_boxes(\n",
    "        image, boxes=predictions[\"boxes\"], labels=labels, width=5, colors=\"red\"\n",
    "    )\n",
    "    img = box.detach()\n",
    "    return FT.to_pil_image(img)\n",
    "    # im = Image.fromarray(im.permute(1, 2, 0).numpy())\n",
    "    # return im\n",
    "\n",
    "\n",
    "img_with_box = draw_boxes(img, predictions, weights.meta[\"categories\"])\n",
    "img_with_box\n",
    "# im.save(DATA_PATH.joinpath(\"ducks_with_box.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431c7d9-f1dd-447e-b421-e2a3b2517e91",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d27167e6ad5bed75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Initialize the model again but with a lower value for [box_score_thresh](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py). \n",
    "\n",
    "Create predictions for \"ducks.jpg\".\n",
    "\n",
    "Visualize the boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c248fee-2e2a-444c-bc54-dde5d5aa2b8d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-37ed2929b2adae96",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "model = Model(weights=weights, box_score_thresh=0.3)\n",
    "model = model.eval()\n",
    "\n",
    "img = FT.pil_to_tensor(img_ducks)\n",
    "predictions = inference_single(img, model, weights.transforms())\n",
    "\n",
    "img_with_boxes = draw_boxes(img, predictions, weights.meta[\"categories\"])\n",
    "display(img_with_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d413efe-4ed4-49c1-b47d-463f22b27102",
   "metadata": {},
   "source": [
    "**Question**: What happens with different values of `box_score_thresh`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb6067-e046-40e4-aff9-8f1ee3eb10b7",
   "metadata": {},
   "source": [
    "Calculate IoU for the detected boxes. Use the following function: [torchvision.ops.box_iou](https://pytorch.org/vision/stable/generated/torchvision.ops.box_iou.html#torchvision.ops.box_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20218d8-ed5b-47db-925a-c709d27c4d27",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3dfb751c58d0c305",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ious = box_iou(predictions[\"boxes\"], predictions[\"boxes\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "_ = sns.heatmap(\n",
    "    ious.detach().numpy(), annot=True, fmt=\".2f\", annot_kws={\"size\": 8}, cbar=False\n",
    ").set(title=\"IoUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e5a81-9862-4085-855d-98800933c3a3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b266289993f18851",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now let’s take a look at the activation maps from the backbone CNN that are passed into the RPN. Use the following function to inspect the shape of the activation map.\n",
    "\n",
    "Examine the activation maps of both example images. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31691f6-2e5c-49fc-bfc6-6fd6c2491eca",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-89997b51c70d45d9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backbone(img, model, preprocess):\n",
    "    \"\"\"Get Features from the Backbone Network\n",
    "    Args:\n",
    "        img: (C, H, W) torch.tensor\n",
    "        model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "        preprocess: function to pre-process image batch for the model\n",
    "\n",
    "    Returns:\n",
    "        predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "    features = model.backbone(image_processed)\n",
    "    return features[\"0\"]\n",
    "\n",
    "\n",
    "model = Model(weights=weights, box_score_thresh=0.5)\n",
    "model = model.eval()\n",
    "\n",
    "img = FT.pil_to_tensor(img_dogs)\n",
    "backbone_features = backbone(img, model, weights.transforms())\n",
    "\n",
    "backbone_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff6dc6-304d-4260-b133-b9c78f51f5c6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-55115421343d1fa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now let’s take a look at the output of the RPN. Compare the two images again.\n",
    "\n",
    "Set the model parameters: `rpn_score_thresh` and `rpn_post_nms_top_n_test`, and experiment with different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60120e6-e264-4ef9-8c4b-1c185ac47d7b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f4bc3ac14d76daf4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rpn(img, model, preprocess):\n",
    "    \"\"\"Get Region Proposals\n",
    "    Args:\n",
    "        img: (C, H, W) torch.tensor\n",
    "        model: torchvision.models.detection.faster_rcnn.FasterRCNN\n",
    "        preprocess: function to pre-process image batch for the model\n",
    "\n",
    "    Returns:\n",
    "        predictions: Dict with lists of object detections\n",
    "    \"\"\"\n",
    "    image_batch = img.unsqueeze(0)\n",
    "    image_processed = preprocess(image_batch)\n",
    "\n",
    "    images, targets = model.transform(image_processed, targets=None)\n",
    "    features = model.backbone(image_processed)\n",
    "    proposals, proposal_losses = model.rpn(images, features, targets=targets)\n",
    "\n",
    "    original_image_sizes: List[Tuple[int, int]] = []\n",
    "    for img in image_batch:\n",
    "        val = img.shape[-2:]\n",
    "        torch._assert(\n",
    "            len(val) == 2,\n",
    "            f\"expecting the last two dimensions of the Tensor to be H and W instead got {img.shape[-2:]}\",\n",
    "        )\n",
    "        original_image_sizes.append((val[0], val[1]))\n",
    "    proposals = model.transform.postprocess(\n",
    "        [{\"boxes\": proposals[0]}], images.image_sizes, original_image_sizes\n",
    "    )\n",
    "\n",
    "    return proposals\n",
    "\n",
    "\n",
    "def draw_proposals(image, proposals):\n",
    "    \"\"\"Draw Boxes from Predictions\n",
    "    Args:\n",
    "        image: The input image torch.tensor\n",
    "        predictions: Output of inference()\n",
    "        categories: List of category labels\n",
    "    Returns:\n",
    "        PIL.Image\n",
    "    \"\"\"\n",
    "\n",
    "    box = draw_bounding_boxes(image, boxes=proposals, width=5, colors=\"red\")\n",
    "    im = box.detach()\n",
    "    im = Image.fromarray(im.permute(1, 2, 0).numpy())\n",
    "    return im\n",
    "\n",
    "\n",
    "model = Model(weights=weights, rpn_score_thresh=0.5, rpn_post_nms_top_n_test=200)\n",
    "model = model.eval()\n",
    "\n",
    "image_torch = torch.tensor(np.array(img_ducks)).permute(2, 0, 1)\n",
    "region_proposals = rpn(image_torch, model, weights.transforms())\n",
    "im = draw_proposals(image_torch, region_proposals[0][\"boxes\"])\n",
    "im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
