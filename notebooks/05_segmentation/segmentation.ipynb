{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39280e4c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a66f2f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Semantic Segmentation: architecture-Design, training and evaluation of models\n",
    "- Upsampling: understand and apply different techniques\n",
    "- Instance Segmentation: Anwenden von Pre-Trained Modellen, Verstehen & Evaluieren der Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d04dc3-451e-4f4c-8703-1f309b96526c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's define paths, install & load the necessary Python packages.\n",
    "\n",
    "**Optional: Save the notebook to your personal google drive to persist changes.**\n",
    "\n",
    "**Optional: Change runtime to a GPU instance (if using Google Colab)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ada99c-4e89-45e3-ada7-bae39434d0f7",
   "metadata": {},
   "source": [
    "Mount your google drive to store data and results (if running the code in Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856fb48-c927-4260-b30d-78e8de40f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb64f2a-1149-48ca-8d37-945d1f331213",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b639f1df-0232-4fd1-8c49-d0db35921b3d",
   "metadata": {},
   "source": [
    "**Modify the following paths if necessary.**\n",
    "\n",
    "That is where your data will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d1794-9ca4-45f6-9bf5-ebae5972d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/bveri\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"/workspace/code/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e033479-3698-4842-ba25-388d5ad7741d",
   "metadata": {},
   "source": [
    "Install `dl_cv_lectures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d44762-7c72-43c7-ba2b-708a41a00021",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dl_cv_lectures\n",
    "\n",
    "    print(\"dl_cv_lectures installed, all good\")\n",
    "except ImportError as e:\n",
    "    import os\n",
    "\n",
    "    if Path(\"/workspace/code/src\").exists():\n",
    "        print(\"Installing from local repo\")\n",
    "        os.system(\"cd /workspace/code  && pip install -e .\")\n",
    "    else:\n",
    "        print(\"Installing from git repo\")\n",
    "        os.system(\"pip install git+https://github.com/i4Ds/bveri-exercises-hs2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd5765-f776-4dc5-9159-9de5530d4c8e",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f36948-f5cc-44ca-a04e-0acc53a8350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchshow as ts\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff74686-bcf4-4d11-8772-eb3807be8e2e",
   "metadata": {},
   "source": [
    "Define a default device for your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad257435-8079-4b99-8ea6-e4f36b1338df",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b1ca8-31c6-4120-98de-20f586f6a227",
   "metadata": {},
   "source": [
    "## Stanford Background Dataset\n",
    "\n",
    "Let's take a look at the [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html).\n",
    "\n",
    "Let's download the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9387e85a-4b5e-4d6c-b1d0-f1a1adf1da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_cv_lectures.data import stanford_background\n",
    "\n",
    "stanford_background.download(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db13d30c-a566-485c-90a7-46058d45fccd",
   "metadata": {},
   "source": [
    "Let's take a look at how the dataset is organized and visualize some images.\n",
    "\n",
    "(this example command only works on Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95844df-bce2-43c1-b218-42f8f67daf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {DATA_PATH}/stanford_background_dataset/ -type d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85718e24-ebbe-47f9-8ecd-cc1f3919c8bf",
   "metadata": {},
   "source": [
    "Let's inspect the images directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb08c7-f142-44a5-84b0-0b1c46af0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {DATA_PATH}/stanford_background_dataset/stanford_background_dataset/images -type f | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7ccda-99ab-4c3f-a686-240a64d80ebd",
   "metadata": {},
   "source": [
    "Let's inspect the labels directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962237ab-f62e-40e6-82a2-2eae767de703",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find {DATA_PATH}/stanford_background_dataset/stanford_background_dataset/labels -type f | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cb6934-a442-4f20-9de7-4597ac464f48",
   "metadata": {},
   "source": [
    "Now we read the first image and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef9d0e-fa1d-4f07-bcf4-bea851f6127e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_sbds = Image.open(\n",
    "    DATA_PATH.joinpath(\n",
    "        \"stanford_background_dataset/stanford_background_dataset/images/0000047.jpg\"\n",
    "    )\n",
    ")\n",
    "label_path = DATA_PATH.joinpath(\n",
    "    \"stanford_background_dataset/stanford_background_dataset/labels/0000047.regions.txt\"\n",
    ")\n",
    "display(img_sbds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fc807-7a98-44a9-9c2a-7308861e1a39",
   "metadata": {},
   "source": [
    "**Task**: Look at a couple of more images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da660af-9aff-4672-8bed-5a6e4536e58b",
   "metadata": {},
   "source": [
    "**Task**: Try to figure out what the \".regions.txt\" files are about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07f564-b899-4bb4-9080-b63ab4b58d4b",
   "metadata": {},
   "source": [
    "## Build a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfb291-5cf4-4a8e-96e9-1eb9b2ce63f1",
   "metadata": {},
   "source": [
    "We define a `torch.utils.data.Dataset` and display an image and it's segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabc0a9-fef8-4e4c-a67a-c0d10fe42586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordBackgroundDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: Path,\n",
    "        transform_images: Callable = None,\n",
    "        transform_labels: Callable = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            root_path (Path): Path to the dataset directory.\n",
    "            transform_images (callable, optional): Transformation function for images.\n",
    "            transform_labels (callable, optional): Transformation function for labels.\n",
    "        \"\"\"\n",
    "        self.root_path = root_path\n",
    "        self.transform_images = transform_images\n",
    "        self.transform_labels = transform_labels\n",
    "        self.image_paths = list((root_path / \"images\").glob(\"*.jpg\"))\n",
    "        self.classes = [\n",
    "            \"sky\",\n",
    "            \"tree\",\n",
    "            \"road\",\n",
    "            \"grass\",\n",
    "            \"water\",\n",
    "            \"building\",\n",
    "            \"mountain\",\n",
    "            \"foreground object\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(\n",
    "        self, idx: int\n",
    "    ) -> tuple[torch.Tensor | Image.Image, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves the image and corresponding label masks for a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - image (torch.Tensor | Image.Image): The transformed image or original image.\n",
    "                - label_masks (torch.Tensor): A binary mask tensor of shape (K, H, W) where K is the number of classes.\n",
    "                  Each channel represents the binary mask for a specific class.\n",
    "                - labels_tensor (torch.Tensor): A segmentation map tensor of shape (1, H, W) indicating class indices\n",
    "                for each pixel.\n",
    "        \"\"\"\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        label_path = self.root_path / f\"labels/{image_path.stem}.regions.txt\"\n",
    "        labels = self._parse_regions(label_path)\n",
    "\n",
    "        labels_tensor = torch.tensor(labels).unsqueeze(0).clamp(0, len(self.classes) - 1)\n",
    "        label_masks = torch.zeros(len(self.classes), *labels.shape).scatter_(0, labels_tensor, 1)\n",
    "\n",
    "        if self.transform_images:\n",
    "            image = self.transform_images(image)\n",
    "        if self.transform_labels:\n",
    "            label_masks = self.transform_labels(label_masks)\n",
    "            labels_tensor = self.transform_labels(labels_tensor)\n",
    "\n",
    "        return image, label_masks, labels_tensor\n",
    "\n",
    "    def _parse_regions(self, path: Path) -> np.ndarray:\n",
    "        with open(path, \"r\") as file:\n",
    "            return np.array([list(map(int, line.split())) for line in file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19abebf-00c0-4b67-a89c-3a86d66dd85d",
   "metadata": {},
   "source": [
    "Create an object of the `StanfordBackgroundDataset` class. \n",
    "\n",
    "Retrieve the first observation and display:\n",
    "\n",
    "- the image\n",
    "- segmentation map\n",
    "- all binary class maps\n",
    "\n",
    "Use `torchshow` for simplicity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266cad3-8527-4276-934c-017956fa4a48",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "dataset",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd282c-9766-4469-9df7-059c4694d5ca",
   "metadata": {},
   "source": [
    "**Task**: Which is the mask for sky? Which the one for trees? How can you find our for sure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0826ee-92b3-4c2a-8a33-0a28f99b2fb8",
   "metadata": {},
   "source": [
    "## Fully-Convolutional Network\n",
    "\n",
    "Implement a fully convolutional network with an encoder-decoder architecture. Complete the following classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453d2c9-60ea-4665-8c9b-f409ffc2680d",
   "metadata": {},
   "source": [
    "We start by implementing an `EncoderBlock`. It should implement the following operations:\n",
    "\n",
    "- **Convolution -> Batch Normalization -> ReLU -> Convolution (stride = 2) -> Batch Normalization -> ReLU**\n",
    "\n",
    "Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217a141-4328-4884-82ee-3c5be0d432ac",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "fcn",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"A basic encoder block that performs convolution, normalization, and activation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride=1, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Implement conv2 and bn2\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b52c6c-ee65-4a4d-9ce6-d7a214ddc9e7",
   "metadata": {},
   "source": [
    "Next, we stack the EncoderBlocks to an `Encoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9645b-f252-44f4-8029-36d7d5336222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes an image to a low-dimensional representation.\n",
    "\n",
    "    Args:\n",
    "        num_channels_in (int): Number of input channels (e.g., 3 for RGB images).\n",
    "        num_channels (list[int]): Number of output channels for each block.\n",
    "            Each block reduces spatial dimensionality by half.\n",
    "    Input:\n",
    "        image batch of shape (N, C, H, W)\n",
    "\n",
    "    Output:\n",
    "        image batch of shape (N, C2, H / S, W / S), where S is the global stride.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels_in: int, num_channels: list[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        num_channels = [num_channels_in] + num_channels\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for in_channels, out_channels in zip(num_channels, num_channels[1:]):\n",
    "            self.layers_.append(EncoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36fc3d1-aeb5-4ceb-b181-e793a0328d46",
   "metadata": {},
   "source": [
    "**Question**: How many Encoder Blocks do you need to reduce a 1024 x 1024 image to a single scalar? How many output-channels would you have if you double the number of channels with each block (first block extracts 16 channels)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5d4f0-9ee6-4650-91af-048dd82f8a84",
   "metadata": {},
   "source": [
    "Now we do the same for the Decoder part. First we implement a Decoder block which upsamples the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f598bd-9789-4a3c-aca6-63f0f2fcb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"A basic decoder block that performs transposed convolution, normalization, and activation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        padding=1,\n",
    "        output_padding=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # transposed convolution layer for upsampling\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c5d3b-dd7d-429a-9ecd-c431e9f0292f",
   "metadata": {},
   "source": [
    "And we combine the blocks to a Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d60043-4679-448e-b00a-7d6dc6200380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decodes a low-dimensional representation back to an image.\n",
    "\n",
    "    Args:\n",
    "        num_channels_in (int): Number of input channels (output of encoder)\n",
    "        num_channels (list[int]): Number of channels for each block, reversed from the encoder configuration.\n",
    "    Input:\n",
    "        feature map of shape (N, C, H, W)\n",
    "    Output:\n",
    "        image batch of shape (N, C_out, H_out, W_out), where C_out is typically the original input channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels_in, num_channels: list[int]):\n",
    "        super().__init__()\n",
    "\n",
    "        num_channels = [num_channels_in] + num_channels\n",
    "\n",
    "        # Reverse the list of channels to create the symmetrical structure\n",
    "        self.layers_ = nn.ModuleList()\n",
    "        for in_channels, out_channels in zip(num_channels, num_channels[1:]):\n",
    "            self.layers_.append(DecoderBlock(in_channels=in_channels, out_channels=out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers_:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae4b97-d283-4bba-a370-af1499d28e59",
   "metadata": {},
   "source": [
    "Now implement the `EncoderDecoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d970cc-6b3d-478c-8e80-553e8b6cb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"Encoder-Decoder architecture for image-to-image tasks.\n",
    "    Args:\n",
    "        encoder (nn.Module): The encoder network that reduces the\n",
    "            spatial dimensions and extracts features from the input.\n",
    "        decoder (nn.Module): The decoder network that upsamples the\n",
    "            features and reconstructs the output image.\n",
    "        num_in_channels (int): Number of input channels (e.g., 3 for RGB images).\n",
    "        num_in_encoder_channels (int): Number of channels for the input\n",
    "            convolution layer, used to match the encoder's initial channel size.\n",
    "        num_out_decoder_channels (int): Number of channels for the final\n",
    "            output from the decoder before the output layer.\n",
    "        num_output_channels (int): Number of output channels (e.g., 1 for grayscale or\n",
    "            3 for RGB).\n",
    "\n",
    "    Input:\n",
    "        x (torch.Tensor): Image batch of shape (N, num_in_channels, H, W).\n",
    "\n",
    "    Output:\n",
    "        torch.Tensor: Processed image batch of shape (N, num_output_channels, H, W).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        num_in_channels,\n",
    "        num_in_encoder_channels,\n",
    "        num_out_decoder_channels,\n",
    "        num_output_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            num_in_encoder_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        # Implement self.output to get the correct number of output channels\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d9aa9e-03a1-4c1e-8b78-baf1cef9e9c8",
   "metadata": {},
   "source": [
    "Überprüfen Sie die Architektur. Z.B. das die Output-Shape korrekt ist. Wir möchten pro Klasse eine eigene Maske erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f74c377-77ac-417b-b3b6-e70df02b8698",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "model",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192bf32-1ba3-4a0d-945c-ac22032fb1e5",
   "metadata": {},
   "source": [
    "### Model-Training and Metrics\n",
    "\n",
    "Now, train the model and monitor your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fdccd-c69f-438a-ab6a-503189e638db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tr_images = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        #   transforms.ConvertImageDtype(torch.float),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tr_labels = transforms.Compose([transforms.CenterCrop((256, 256))])\n",
    "\n",
    "ds = StanfordBackgroundDataset(root_path, transform_images=tr_images, transform_labels=tr_labels)\n",
    "ds_loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6363f-ed42-4a47-87f9-33c0de47e1a4",
   "metadata": {},
   "source": [
    "Complete the calculation of pixel accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b36226-e241-4d4e-938d-672ef6615fd6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "train",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 8\n",
    "\n",
    "# create model\n",
    "num_classes = len(ds.classes)\n",
    "num_channels_encoder = [16, 32, 64]\n",
    "num_channels_decoder = [32, 16, 8]\n",
    "\n",
    "encoder = Encoder(num_channels_in=8, num_channels=num_channels_encoder)\n",
    "decoder = Decoder(num_channels_in=64, num_channels=num_channels_decoder)\n",
    "encoder_decoder = EncoderDecoder(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    num_in_channels=3,\n",
    "    num_output_channels=num_classes,\n",
    "    num_in_encoder_channels=8,\n",
    "    num_out_decoder_channels=8,\n",
    ")\n",
    "\n",
    "encoder_decoder = encoder_decoder.to(device)\n",
    "\n",
    "# Create Loss-Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(encoder_decoder.parameters())\n",
    "\n",
    "pbar = tqdm(total=num_epochs * len(ds_loader))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(0, num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(ds_loader):\n",
    "\n",
    "        images, label_masks, label_images = data\n",
    "\n",
    "        # Forward-Pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = encoder_decoder(images.to(device))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        # Optimize\n",
    "        loss = criterion(logits, label_masks.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate Pixel-Accuracy - calculate predicted class per pixel and compare\n",
    "        # to ground truth class\n",
    "        # pred =\n",
    "        # pixel_acc =\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        running_acc += pixel_acc\n",
    "        step += 1\n",
    "        print_every = 10\n",
    "        if (i % print_every) == (print_every - 1):\n",
    "            desc = f\"Epoch: {epoch + 1}, Iteration: {i + 1:5d}] Loss: {running_loss / print_every:.3f} Acc: {running_acc / print_every:.3f}\"\n",
    "            _ = pbar.update(print_every)\n",
    "            _ = pbar.set_description(desc)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "pbar.close()\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334187-7e84-499f-9c67-bd763d6ea59e",
   "metadata": {},
   "source": [
    "Visualize the prediction on one image and commpare with the annotated _segmentation map_. Use `torchshow` for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab730b9c-e4ac-4016-ba19-6c36da815361",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "pred",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = encoder_decoder(images.to(device))\n",
    "probs = F.softmax(logits, dim=1)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d3a6d-4dc7-4b4e-abe3-ab515fa2ce01",
   "metadata": {},
   "source": [
    "## Upsampling\n",
    "\n",
    "We now look at a few upsampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052fb05-1493-4507-9c98-355da913483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upsample = torch.tensor([[1, 2], [3, 4]]).unsqueeze(0).to(torch.float)\n",
    "\n",
    "to_upsample_2 = torch.concat([to_upsample, to_upsample], dim=2)\n",
    "\n",
    "to_upsample_2 = torch.concat([to_upsample_2, to_upsample_2], dim=1)\n",
    "\n",
    "\n",
    "def display_arrays(arrays: list[np.ndarray], titles: list[str]):\n",
    "    \"\"\"Display Arrays\"\"\"\n",
    "    num_arrays = len(arrays)\n",
    "    kwargs = {\n",
    "        \"annot\": True,\n",
    "        \"cbar\": False,\n",
    "        \"vmin\": 0,\n",
    "        \"vmax\": 10,\n",
    "        \"xticklabels\": False,\n",
    "        \"yticklabels\": False,\n",
    "    }\n",
    "    fig, ax = plt.subplots(figsize=(3 * num_arrays, 3), ncols=num_arrays)\n",
    "\n",
    "    # handle single and multi-array plots\n",
    "    if num_arrays > 1:\n",
    "        axes = ax.flatten()\n",
    "    else:\n",
    "        axes = [ax]\n",
    "\n",
    "    for i, (array, title) in enumerate(zip(arrays, titles)):\n",
    "        sns.heatmap(array, **kwargs, ax=axes[i]).set(title=f\"{title} - Shape {array.shape}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_arrays([np.array(to_upsample[0, :, :])], [\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a743e-9145-4cc8-b3f7-775913163d79",
   "metadata": {},
   "source": [
    "### Unpooling\n",
    "\n",
    "Test max-pooling and -unpooling using a switch. Test different parameters and take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790330a-c843-414e-925d-618752237788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "to_upsample_3 = torch.clone(to_upsample_2)\n",
    "to_upsample_3[0, 0, 0] = 16\n",
    "\n",
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "output, indices = pool(to_upsample_3)\n",
    "unpooled = unpool(output, indices)\n",
    "\n",
    "display_arrays(\n",
    "    arrays=[\n",
    "        np.array(to_upsample_3[0, :, :]),\n",
    "        np.array(output[0, :, :]),\n",
    "        np.array(unpooled[0, :, :]),\n",
    "    ],\n",
    "    titles=[\"input\", \"pooled\", \"unpooled\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2145386-bcff-4841-8e88-ee0db8c113e7",
   "metadata": {},
   "source": [
    "### Transposed Convolution\n",
    "\n",
    "Test different parameters for _transposed convolution_. Create two more variants and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97d909-5acb-4839-9b30-64d67c938c80",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "trans_conv",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "weight = torch.tensor([[1, 2, 3], [0, 1, 2], [0, 1, 2]]).unsqueeze(0).unsqueeze(0).to(torch.float)\n",
    "weight.shape\n",
    "\n",
    "input_ = to_upsample_2\n",
    "\n",
    "out = F.conv_transpose2d(input=input_, weight=weight, stride=2, padding=0, output_padding=0)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "arrays_to_plot = [\n",
    "    np.array(x) for x in [input_[0, ::], weight[0, 0, ::], out[0, ::], out2[0, ::], out3[0, ::]]\n",
    "]\n",
    "\n",
    "display_arrays(arrays=arrays_to_plot, titles=[\"Input\", \"Filter\", \"Output\", \"Output2\", \"Output3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
